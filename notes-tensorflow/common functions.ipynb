{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]]\n",
      "[[1 1 1]\n",
      " [1 1 1]]\n",
      "[[ 3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "a=tf.zeros([2,3],tf.int32)\n",
    "b=tf.ones([2,3],tf.int32)\n",
    "c=tf.constant([[3., 3.]])\n",
    "print(sess.run(a))\n",
    "print(sess.run(b))    \n",
    "print(sess.run(c)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'random_uniform:0' shape=(2, 3) dtype=float32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random_normal(shape=[2,3], mean=0.0, stddev=1.0)\n",
    "tf.random_uniform(shape=[2,3], minval=0.0, maxval=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function random_normal in module tensorflow.python.ops.random_ops:\n",
      "\n",
      "random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
      "    Outputs random values from a normal distribution.\n",
      "    \n",
      "    Args:\n",
      "      shape: A 1-D integer Tensor or Python array. The shape of the output tensor.\n",
      "      mean: A 0-D Tensor or Python value of type `dtype`. The mean of the normal\n",
      "        distribution.\n",
      "      stddev: A 0-D Tensor or Python value of type `dtype`. The standard deviation\n",
      "        of the normal distribution.\n",
      "      dtype: The type of the output.\n",
      "      seed: A Python integer. Used to create a random seed for the distribution.\n",
      "        See\n",
      "        [`set_random_seed`](../../api_docs/python/constant_op.md#set_random_seed)\n",
      "        for behavior.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A tensor of the specified shape filled with random normal values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.random_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 损失函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-df2436d35ba0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorss_entroy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_' is not defined"
     ]
    }
   ],
   "source": [
    "#交叉熵\n",
    "#二分类\n",
    "cross_entroy=tf.nn.sigmoid_cross_entropy_with_logits(y,y_)\n",
    "#多分类\n",
    "cross_entroy=tf.nn.softmax_cross_entropy_with_logits(y,y_)\n",
    "#自定义\n",
    "cross_entroy=-tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,le-10,1.0)))\n",
    "loss=tf.reuce_sum(tf.select(tf.greater(v1,v2),(v1-v2)*1,(v2-v1)*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#L1 , L2 ,lambda为正则化项系数\n",
    "l2=tf.contrib.layers.l1_regularizer(lambda)(w)\n",
    "l2=tf.contrib.layers.l2_regularizer(lambda)(w)\n",
    "loss=tf.reduce_mean(tf.reduce_mean(tf.square(y-y_)+ l2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'leanring_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-d0906a4417a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleanring_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleanring_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'leanring_rate' is not defined"
     ]
    }
   ],
   "source": [
    "tf.train.GradientDescentOptimizer(leanring_rate).minimize(loss)\n",
    "tf.train.AdamOptimizer(leanring_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 衰减率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ExponentialMovingAverage in module tensorflow.python.training.moving_averages:\n",
      "\n",
      "class ExponentialMovingAverage(builtins.object)\n",
      " |  Maintains moving averages of variables by employing an exponential decay.\n",
      " |  \n",
      " |  When training a model, it is often beneficial to maintain moving averages of\n",
      " |  the trained parameters.  Evaluations that use averaged parameters sometimes\n",
      " |  produce significantly better results than the final trained values.\n",
      " |  \n",
      " |  The `apply()` method adds shadow copies of trained variables and add ops that\n",
      " |  maintain a moving average of the trained variables in their shadow copies.\n",
      " |  It is used when building the training model.  The ops that maintain moving\n",
      " |  averages are typically run after each training step.\n",
      " |  The `average()` and `average_name()` methods give access to the shadow\n",
      " |  variables and their names.  They are useful when building an evaluation\n",
      " |  model, or when restoring a model from a checkpoint file.  They help use the\n",
      " |  moving averages in place of the last trained values for evaluations.\n",
      " |  \n",
      " |  The moving averages are computed using exponential decay.  You specify the\n",
      " |  decay value when creating the `ExponentialMovingAverage` object.  The shadow\n",
      " |  variables are initialized with the same initial values as the trained\n",
      " |  variables.  When you run the ops to maintain the moving averages, each\n",
      " |  shadow variable is updated with the formula:\n",
      " |  \n",
      " |    `shadow_variable -= (1 - decay) * (shadow_variable - variable)`\n",
      " |  \n",
      " |  This is mathematically equivalent to the classic formula below, but the use\n",
      " |  of an `assign_sub` op (the `\"-=\"` in the formula) allows concurrent lockless\n",
      " |  updates to the variables:\n",
      " |  \n",
      " |    `shadow_variable = decay * shadow_variable + (1 - decay) * variable`\n",
      " |  \n",
      " |  Reasonable values for `decay` are close to 1.0, typically in the\n",
      " |  multiple-nines range: 0.999, 0.9999, etc.\n",
      " |  \n",
      " |  Example usage when creating a training model:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Create variables.\n",
      " |  var0 = tf.Variable(...)\n",
      " |  var1 = tf.Variable(...)\n",
      " |  # ... use the variables to build a training model...\n",
      " |  ...\n",
      " |  # Create an op that applies the optimizer.  This is what we usually\n",
      " |  # would use as a training op.\n",
      " |  opt_op = opt.minimize(my_loss, [var0, var1])\n",
      " |  \n",
      " |  # Create an ExponentialMovingAverage object\n",
      " |  ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n",
      " |  \n",
      " |  # Create the shadow variables, and add ops to maintain moving averages\n",
      " |  # of var0 and var1.\n",
      " |  maintain_averages_op = ema.apply([var0, var1])\n",
      " |  \n",
      " |  # Create an op that will update the moving averages after each training\n",
      " |  # step.  This is what we will use in place of the usual training op.\n",
      " |  with tf.control_dependencies([opt_op]):\n",
      " |      training_op = tf.group(maintain_averages_op)\n",
      " |  \n",
      " |  ...train the model by running training_op...\n",
      " |  ```\n",
      " |  \n",
      " |  There are two ways to use the moving averages for evaluations:\n",
      " |  \n",
      " |  *  Build a model that uses the shadow variables instead of the variables.\n",
      " |     For this, use the `average()` method which returns the shadow variable\n",
      " |     for a given variable.\n",
      " |  *  Build a model normally but load the checkpoint files to evaluate by using\n",
      " |     the shadow variable names.  For this use the `average_name()` method.  See\n",
      " |     the [Saver class](../../api_docs/python/train.md#Saver) for more\n",
      " |     information on restoring saved variables.\n",
      " |  \n",
      " |  Example of restoring the shadow variable values:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Create a Saver that loads variables from their saved shadow values.\n",
      " |  shadow_var0_name = ema.average_name(var0)\n",
      " |  shadow_var1_name = ema.average_name(var1)\n",
      " |  saver = tf.train.Saver({shadow_var0_name: var0, shadow_var1_name: var1})\n",
      " |  saver.restore(...checkpoint filename...)\n",
      " |  # var0 and var1 now hold the moving average values\n",
      " |  ```\n",
      " |  \n",
      " |  @@__init__\n",
      " |  @@apply\n",
      " |  @@average_name\n",
      " |  @@average\n",
      " |  @@variables_to_restore\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, decay, num_updates=None, name='ExponentialMovingAverage')\n",
      " |      Creates a new ExponentialMovingAverage object.\n",
      " |      \n",
      " |      The `apply()` method has to be called to create shadow variables and add\n",
      " |      ops to maintain moving averages.\n",
      " |      \n",
      " |      The optional `num_updates` parameter allows one to tweak the decay rate\n",
      " |      dynamically. It is typical to pass the count of training steps, usually\n",
      " |      kept in a variable that is incremented at each step, in which case the\n",
      " |      decay rate is lower at the start of training.  This makes moving averages\n",
      " |      move faster.  If passed, the actual decay rate used is:\n",
      " |      \n",
      " |        `min(decay, (1 + num_updates) / (10 + num_updates))`\n",
      " |      \n",
      " |      Args:\n",
      " |        decay: Float.  The decay to use.\n",
      " |        num_updates: Optional count of number of updates applied to variables.\n",
      " |        name: String. Optional prefix name to use for the name of ops added in\n",
      " |          `apply()`.\n",
      " |  \n",
      " |  apply(self, var_list=None)\n",
      " |      Maintains moving averages of variables.\n",
      " |      \n",
      " |      `var_list` must be a list of `Variable` or `Tensor` objects.  This method\n",
      " |      creates shadow variables for all elements of `var_list`.  Shadow variables\n",
      " |      for `Variable` objects are initialized to the variable's initial value.\n",
      " |      They will be added to the `GraphKeys.MOVING_AVERAGE_VARIABLES` collection.\n",
      " |      For `Tensor` objects, the shadow variables are initialized to 0 and zero\n",
      " |      debiased (see docstring in `assign_moving_average` for more details).\n",
      " |      \n",
      " |      shadow variables are created with `trainable=False` and added to the\n",
      " |      `GraphKeys.ALL_VARIABLES` collection.  They will be returned by calls to\n",
      " |      `tf.all_variables()`.\n",
      " |      \n",
      " |      Returns an op that updates all shadow variables as described above.\n",
      " |      \n",
      " |      Note that `apply()` can be called multiple times with different lists of\n",
      " |      variables.\n",
      " |      \n",
      " |      Args:\n",
      " |        var_list: A list of Variable or Tensor objects. The variables\n",
      " |          and Tensors must be of types float16, float32, or float64.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An Operation that updates the moving averages.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the arguments are not all float16, float32, or float64.\n",
      " |        ValueError: If the moving average of one of the variables is already\n",
      " |          being computed.\n",
      " |  \n",
      " |  average(self, var)\n",
      " |      Returns the `Variable` holding the average of `var`.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: A `Variable` object.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Variable` object or `None` if the moving average of `var`\n",
      " |        is not maintained.\n",
      " |  \n",
      " |  average_name(self, var)\n",
      " |      Returns the name of the `Variable` holding the average for `var`.\n",
      " |      \n",
      " |      The typical scenario for `ExponentialMovingAverage` is to compute moving\n",
      " |      averages of variables during training, and restore the variables from the\n",
      " |      computed moving averages during evaluations.\n",
      " |      \n",
      " |      To restore variables, you have to know the name of the shadow variables.\n",
      " |      That name and the original variable can then be passed to a `Saver()` object\n",
      " |      to restore the variable from the moving average value with:\n",
      " |        `saver = tf.train.Saver({ema.average_name(var): var})`\n",
      " |      \n",
      " |      `average_name()` can be called whether or not `apply()` has been called.\n",
      " |      \n",
      " |      Args:\n",
      " |        var: A `Variable` object.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A string: The name of the variable that will be used or was used\n",
      " |        by the `ExponentialMovingAverage class` to hold the moving average of\n",
      " |        `var`.\n",
      " |  \n",
      " |  variables_to_restore(self, moving_avg_variables=None)\n",
      " |      Returns a map of names to `Variables` to restore.\n",
      " |      \n",
      " |      If a variable has a moving average, use the moving average variable name as\n",
      " |      the restore name; otherwise, use the variable name.\n",
      " |      \n",
      " |      For example,\n",
      " |      \n",
      " |      ```python\n",
      " |        variables_to_restore = ema.variables_to_restore()\n",
      " |        saver = tf.train.Saver(variables_to_restore)\n",
      " |      ```\n",
      " |      \n",
      " |      Below is an example of such mapping:\n",
      " |      \n",
      " |      ```\n",
      " |        conv/batchnorm/gamma/ExponentialMovingAverage: conv/batchnorm/gamma,\n",
      " |        conv_4/conv2d_params/ExponentialMovingAverage: conv_4/conv2d_params,\n",
      " |        global_step: global_step\n",
      " |      ```\n",
      " |      Args:\n",
      " |        moving_avg_variables: a list of variables that require to use of the\n",
      " |          moving variable name to be restored. If None, it will default to\n",
      " |          variables.moving_average_variables() + variables.trainable_variables()\n",
      " |      \n",
      " |      Returns:\n",
      " |        A map from restore_names to variables. The restore_name can be the\n",
      " |        moving_average version of the variable name if it exist, or the original\n",
      " |        variable name.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.train.ExponentialMovingAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n",
      "[5.0, 4.5]\n",
      "[10.0, 9.4499998]\n"
     ]
    }
   ],
   "source": [
    "#生成一个指数平滑对象\n",
    "steps=tf.Variable(0)\n",
    "ema=tf.train.ExponentialMovingAverage(decay=0.99,num_updates=steps)\n",
    "#apply\n",
    "v1=tf.Variable(0,dtype=tf.float32) \n",
    "maintain_averages_op =ema.apply([v1])\n",
    "# 初始化向量\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op) \n",
    "    # 通过 ema.average(v1)获取平滑之后的变量的值\n",
    "    print(sess.run([v1,ema.average(v1)])) #[0.0, 0.0]\n",
    "    \n",
    "    # 将v1值更新到5\n",
    "    sess.run(tf.assign(v1,5)) \n",
    "    sess.run(maintain_averages_op)   \n",
    "    print(sess.run([v1,ema.average(v1)])) #[5.0, 4.5]\n",
    "    \n",
    "    # step =1000\n",
    "    sess.run(tf.assign(v1,10)) \n",
    "    sess.run(tf.assign(steps,1000))   # decay = 0.99\n",
    "    sess.run(maintain_averages_op)\n",
    "    print(sess.run([v1,ema.average(v1)])) #[10.0, 4.5549998]\n",
    "    # 0.99 * 4.5 + 0.01 * 10=4.55  # 0.99 * 4.5 , 4.5是上次shadow variable的值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0, 5.0]\n",
      "[10.0, 10.0]\n",
      "[5.0, 5.0]\n",
      "[10.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "v2=tf.Variable(5 ,dtype=tf.float32)\n",
    "v3=tf.Variable(10,dtype=tf.float32)\n",
    "ema=tf.train.ExponentialMovingAverage(decay=0.9,num_updates=steps)\n",
    "maintain_averages_op =ema.apply([v2,v3])\n",
    "# 初始化向量\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op) \n",
    "    # 通过 ema.average(v1)获取平滑之后的变量的值\n",
    "    print(sess.run([v2,ema.average(v2)])) #[0.0, 0.0]\n",
    "    print(sess.run([v3,ema.average(v3)]))\n",
    "    \n",
    "    # 通过 ema.average(v1)获取平滑之后的变量的值\n",
    "    v2=tf.assign(v2 ,10)\n",
    "    print(sess.run([v2,ema.average(v2)])) #[0.0, 0.0] \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0\n",
      "0.1\n",
      "0\n",
      "0.1\n",
      "0\n",
      "0.1\n",
      "0\n",
      "0.1\n",
      "0\n",
      "0.1\n",
      "0\n",
      "0.1\n",
      "0\n",
      "0.1\n",
      "0\n",
      "0.1\n",
      "0\n",
      "0.1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(0.1, global_step, 10, 2, staircase=False)\n",
    "init_op = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init_op)\n",
    "    for i in range(10):\n",
    "        print(sess.run(learning_rate))  \n",
    "        print(sess.run(global_step)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.107177\n",
      "1\n",
      "0.11487\n",
      "2\n",
      "0.123114\n",
      "3\n",
      "0.131951\n",
      "4\n",
      "0.141421\n",
      "5\n",
      "0.151572\n",
      "6\n",
      "0.16245\n",
      "7\n",
      "0.17411\n",
      "8\n",
      "0.186607\n",
      "9\n",
      "0.2\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = tf.placeholder(tf.float32, shape=[None, 1], name='x')  \n",
    "y = tf.placeholder(tf.float32, shape=[None, 1], name='y')  \n",
    "w = tf.Variable(tf.constant(0.0))  \n",
    "  \n",
    "global_step = tf.Variable(0, trainable=False)  \n",
    "learning_rate = tf.train.exponential_decay(0.1, global_step, 10, 2, staircase=False)  \n",
    "loss = tf.pow(w*x-y, 2)  \n",
    "  \n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)  \n",
    "  \n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer())  \n",
    "    for i in range(10):  \n",
    "        sess.run(train_step, feed_dict={x:np.linspace(1,2,10).reshape([10,1]),  \n",
    "            y:np.linspace(1,2,10).reshape([10,1])})  \n",
    "        print(sess.run(learning_rate)  )\n",
    "        print(sess.run(global_step) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Variable' object is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-28e2d713db6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvariable_averages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExponentialMovingAverage\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mvariables_averages_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariable_averages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m    349\u001b[0m       \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[0mzero_debias_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# set of vars to set `zero_debias=True`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m       if var.dtype.base_dtype not in [dtypes.float16, dtypes.float32,\n\u001b[0;32m    353\u001b[0m                                       dtypes.float64]:\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    370\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0minvoked\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m     \"\"\"\n\u001b[1;32m--> 372\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'Variable' object is not iterable.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Variable' object is not iterable."
     ]
    }
   ],
   "source": [
    "global_steps = tf.Variable(0, trainable=False)  \n",
    "learning_rate = tf.train.exponential_decay(0.1, global_steps, 10, 2, staircase=False)  \n",
    "\n",
    "variable_averages = tf.train.ExponentialMovingAverage( 0.99)\n",
    "variables_averages_op = variable_averages.apply(global_steps)\n",
    "\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    sess.run([learning_rate,variables_averages_op])\n",
    "    for i in range(10):     \n",
    "        print(sess.run(learning_rate)  )\n",
    "        print(sess.run(global_steps) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of a new variable (Variable_7:0) must be fully defined, but instead was <unknown>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-6277b538f54a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[0;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m           use_resource=use_resource)\n\u001b[0m\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minitializing_from_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n\u001b[1;32m--> 674\u001b[1;33m                        \"but instead was %s.\" % (name, shape))\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[1;31m# Create the tensor to initialize the variable with default value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of a new variable (Variable_7:0) must be fully defined, but instead was <unknown>."
     ]
    }
   ],
   "source": [
    "a=tf.trainable_variables()[1].name\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.get_variable(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parent directory of /model/model.ckpt doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-650338c79e82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"/model/model.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\anaconda2\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1312\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIsDirectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m       raise ValueError(\n\u001b[1;32m-> 1314\u001b[1;33m           \"Parent directory of {} doesn't exist, can't save.\".format(save_path))\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parent directory of /model/model.ckpt doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "#保存\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.save(sess,\"path/model/model.ckpt\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File path/model/model.ckpt/model.ckpt.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-0b4920965fb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 若只加载图\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msaver2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path/model/model.ckpt/model.ckpt.meta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"/model/model.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda2\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m   \"\"\"\n\u001b[0;32m   1518\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1519\u001b[1;33m     \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1520\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1521\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda2\\envs\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    404\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File %s does not exist.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m   \u001b[1;31m# First try to read it as a binary file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: File path/model/model.ckpt/model.ckpt.meta does not exist."
     ]
    }
   ],
   "source": [
    "# load\n",
    "saver = tf.train.Saver()\n",
    "# 若只加载图\n",
    "saver2 = tf.train.import_meta_graph(\"path/model/model.ckpt/model.ckpt.meta\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,\"/model/model.ckpt\") \n",
    "    saver2.restore(sess,\"/model/model.ckpt\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "1\n",
      "11\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "q=tf.FIFOQueue(2,\"int32\")\n",
    "init=q.enqueue_many(([0,10],))\n",
    "#\n",
    "x=q.dequeue()\n",
    "y=x+1\n",
    "q_inc=q.enqueue([y])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(5):\n",
    "        v,i=sess.run([x,q_inc])\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.]\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"test\"):\n",
    "    v1 = tf.get_variable(name='v1', shape=[1], initializer=tf.constant_initializer(1))   \n",
    "    v2 = tf.get_variable(name='v2', shape=[1], initializer=tf.constant_initializer(2)) \n",
    "\n",
    "tf.add_to_collection('a', v1)\n",
    "tf.add_to_collection('a', v2) \n",
    "c = tf.add_n(tf.get_collection('a') ) \n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init)  \n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_2:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2) \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable_3:0\n"
     ]
    }
   ],
   "source": [
    "v=tf.Variable(1,dtype=tf.int32)\n",
    "print(v.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.]\n",
      "[ 2.]\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"test\",reuse=True):\n",
    "    v=tf.get_variable(\"v\",shape=[1],initializer=tf.constant_initializer(1))\n",
    "#update\n",
    "with tf.Session() as sess:\n",
    "    sess.run(v.initializer)\n",
    "    print(sess.run(v))\n",
    "    #update\n",
    "    sess.run(v.assign([2]))\n",
    "    print(sess.run(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(v1.initializer)\n",
    "    print(sess.run(v1))\n",
    "    # update\n",
    "    sess.run(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
